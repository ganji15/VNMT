Namespace(batch_size=72, brnn=True, brnn_merge='concat', cuda=1, curriculum=False, data='/home/jgt275/VNMT/data/200k_LM.pt-train.pt', dropout=0.3, epochs=1, gamma=0.99, gpus=[0], input_feed=1, lam=1.0, latent_vec_size=100, layers=2, learning_rate=1.0, learning_rate_decay=0.5, log_interval=50, logdir='runs/', max_grad_norm=5, max_len_latent=64, optim='sgd', param_init=0.1, pre_word_vecs_dec=None, pre_word_vecs_enc=None, rnn_size=100, sample=5, sample_reinforce=1, save_model='model', start_decay_at=8, start_epoch=1, train_from=None, word_vec_size=100)
Loading data from '/home/jgt275/VNMT/data/200k_LM.pt-train.pt'
 * vocabulary size. source = 50004; target = 50004
 * number of training sentences. 200000
 * maximum batch size. 72
Building model...
* number of parameters: 15771899
NMTModel (
  (lengthnet): LengthNet (
    (attention): ConvexCombination (
      (sm): Softmax ()
      (linear_in): Linear (100 -> 1)
    )
    (linear): FeedForward (
      (linear_in): Linear (100 -> 82)
      (activation): Tanh ()
      (linear_out): Linear (82 -> 64)
    )
    (sm): Softmax ()
  )
  (encoder): Encoder (
    (word_lut): Embedding(50004, 100, padding_idx=0)
    (rnn): LSTM(100, 50, num_layers=2, dropout=0.3, bidirectional=True)
  )
  (decoder): Decoder (
    (word_lut): Embedding(50004, 100, padding_idx=0)
    (rnn): StackedLSTM (
      (dropout): Dropout (p = 0.3)
      (layer_0): LSTMCell(200, 100)
      (layer_1): LSTMCell(100, 100)
    )
    (attn): GlobalAttention (
      (linear_in): Linear (100 -> 100)
      (sm): Softmax ()
      (linear_out): Linear (200 -> 100)
      (tanh): Tanh ()
    )
    (dropout): Dropout (p = 0.3)
  )
  (decoder_l): DecoderLatent (
    (generator): GeneratorLatent (
      (activation): Tanh ()
      (sigma): Linear (100 -> 100)
      (mu): Linear (100 -> 100)
    )
    (rnn): StackedLSTM (
      (dropout): Dropout (p = 0.3)
      (layer_0): LSTMCell(201, 100)
      (layer_1): LSTMCell(100, 100)
    )
    (attn): GlobalAttentionLatent (
      (linear_in): Linear (100 -> 100)
      (sm): Softmax ()
      (tanh): Tanh ()
    )
    (dropout): Dropout (p = 0.3)
  )
  (encoder_l): EncoderLatent (
    (rnn): LSTM(100, 50, num_layers=2, dropout=0.3, bidirectional=True)
  )
  (generator): Sequential (
    (0): Linear (100 -> 50004)
    (1): LogSoftmax ()
  )
)

Epoch  1,    50/ 2777 batches; perplexity: 10256869.61; 716 Source tokens/s;    118 s elapsed
